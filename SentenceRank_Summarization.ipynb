{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentenceRank-Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashitadutta/FinancialNarrativeSummarization/blob/main/SentenceRank_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRgsh0n0rLjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec3c7ba-a92e-4746-dbb8-e87aa216de38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V5Zg5k9rhsd"
      },
      "source": [
        "\n",
        "### importing the necessary libraries\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import networkx as nx\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5M5_AV9sbAh",
        "outputId": "e6614846-b76f-47bb-8a23-0b20644896d8"
      },
      "source": [
        " import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSEiys8zrfQe",
        "outputId": "b6a63c77-9c53-44c1-840a-2d81462c8678"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4-u_fVNrtig"
      },
      "source": [
        "inp_data=\"/content/drive/MyDrive/lstm-input/\"\n",
        "out_data=\"/content/drive/MyDrive/lstm-summ/\"\n",
        "\n",
        "datasets={\"text\":inp_data,\"summ\":out_data}\n",
        "\n",
        "data_categories=[\"training\",\"validation\",\"test\"]\n",
        "\n",
        "data={\"articles\":[],\"summaries\":[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRxNPoMu4ZZH",
        "outputId": "2ebca466-763d-48cf-d20b-bb9c16f855e0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEL_sEK85S1s",
        "outputId": "339f0e8e-e7f3-4eda-cdb0-e65e40a9a592"
      },
      "source": [
        "!pip install Rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Rouge) (1.15.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvIAShxqZA6B"
      },
      "source": [
        "!unzip training.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6sax3_Ar8QY",
        "outputId": "72ba7abb-83c8-4071-c07d-d80041ad80bd"
      },
      "source": [
        "def parsetext(dire,category,filename):\n",
        "    with open(\"%s/%s\"%(dire,filename),'r',encoding=\"Latin-1\") as readin:\n",
        "        print(\"file read successfully\")\n",
        "        text=readin.read()\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def load_data(dire,category):\n",
        "    for (root,dirs,files) in os.walk(dire):\n",
        "         filenames=sorted(files)\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def cleantext(text):\n",
        "    text=re.sub(r\"what's\",\"what is \",text)\n",
        "    text=re.sub(r\"it's\",\"it is \",text)\n",
        "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
        "    text=re.sub(r\"i'm\",\"i am \",text)\n",
        "    text=re.sub(r\"\\'re\",\" are \",text)\n",
        "    text=re.sub(r\"n't\",\" not \",text)\n",
        "    text=re.sub(r\"\\'d\",\" would \",text)\n",
        "    text=re.sub(r\"\\'s\",\"s\",text)\n",
        "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
        "    text=re.sub(r\"can't\",\" cannot \",text)\n",
        "    text=re.sub(r\" e g \",\" eg \",text)\n",
        "    text=re.sub(r\"e-mail\",\"email\",text)\n",
        "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
        "    text=re.sub(r\" u.s\",\" american \",text)\n",
        "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
        "    text=re.sub(r\"\\n\",\" \",text)\n",
        "    text=re.sub(r\":\",\" \",text)\n",
        "    text=re.sub(r\"-\",\" \",text)\n",
        "    text=re.sub(r\"\\_\",\" \",text)\n",
        "    text=re.sub(r\"\\d+\",\" \",text)\n",
        "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def printArticlesum(k):\n",
        "    print(\"---------------------original sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"articles\"][k])\n",
        "    print(\"----------------------Summary sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"summaries\"][k])\n",
        "    return 0\n",
        "\n",
        "\n",
        "def announcedone():\n",
        "    duration=2000\n",
        "    freq=440\n",
        "    ws.Beep(freq,duration)\n",
        "\n",
        "\n",
        "f1_score1=[]\n",
        "precision1=[]\n",
        "recall1=[]\n",
        "f1_score2=[]\n",
        "precision2=[]\n",
        "recall2=[]\n",
        "f1_scoreL=[]\n",
        "precisionL=[]\n",
        "recallL=[]\n",
        "filenames1=load_data(datasets[\"text\"],data_categories[0])\n",
        "filenames2=load_data(datasets[\"summ\"],data_categories[0])\n",
        "print(filenames1)\n",
        "print(filenames2)\n",
        "\n",
        "\"\"\"----------load the data, sentences and summaries-----------\"\"\"\n",
        "data[\"articles\"]=[]\n",
        "data[\"summaries\"]=[]\n",
        "k=0\n",
        "#while k<len(filenames1):\n",
        "while k<10:\n",
        " try:\n",
        "    print(filenames1[k])\n",
        "    data[\"articles\"].append(parsetext(datasets[\"text\"],data_categories[0],\"%s\"%filenames1[k]))\n",
        " except Exception as e:\n",
        "    data[\"articles\"].append(\"Could not read\")\n",
        "    print(e)\n",
        " try:\n",
        "   print(filenames2[k])\n",
        "   data[\"summaries\"].append(parsetext(datasets[\"summ\"],data_categories[0],\"%s\"%filenames2[k]))\n",
        " except Exception as e:\n",
        "   data[\"summaries\"].append(\"Could not read\")\n",
        "   print(e)\n",
        " \n",
        " reference=\"\"\n",
        " reference=data[\"summaries\"][k]\n",
        " print(\"number of words in reference summary\",len(reference))\n",
        " dict = {}\n",
        " s = \"\"\n",
        " for a in data['articles'][k]:\n",
        "      s += a\n",
        " print(\"number of words in text file\",len(s))\n",
        " \n",
        " # print s\n",
        " s = s.lower()\n",
        " # print s\n",
        "\n",
        " sentences = sent_tokenize(s)\n",
        " print(\"number of sentences in text file\",len(sentences))\n",
        " # print sentences\n",
        "\n",
        " final = []\n",
        "\n",
        " for s in sentences:\n",
        "      temp = re.sub(\"[^a-zA-Z0-9]\",\" \",s)\n",
        "      temp = temp.lower()\n",
        "      final.append(temp)\n",
        "      dict[temp] = s\n",
        " # printfinal\n",
        " def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "    return similarity_matrix\n",
        " stop_words = stopwords.words('english')\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        " sentence_similarity_martix = build_similarity_matrix(final, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        " sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        " scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        " ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(final)), reverse=True)    \n",
        " model_out=\"\"\n",
        " for i in range(5):\n",
        "     model_out+=dict[ranked_sentence[i][1]]\n",
        "     #print(dict[ranked_sentence[i][1]])\n",
        " from rouge import Rouge\n",
        " rouge = Rouge()\n",
        " print(\"number of words in the model summary\",len(model_out))\n",
        " scores = rouge.get_scores(model_out, reference)\n",
        " scores_rouge1 = [score['rouge-1']['r'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['r'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['r'] for score in scores]                 \n",
        " recall1.append(scores_rouge1[0])\n",
        " recall2.append(scores_rouge2[0])\n",
        " recallL.append(scores_rougeL[0])\n",
        " scores_rouge1 = [score['rouge-1']['p'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['p'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['p'] for score in scores]                 \n",
        " precision1.append(scores_rouge1[0])\n",
        " precision2.append(scores_rouge2[0])\n",
        " precisionL.append(scores_rougeL[0])\n",
        " scores_rouge1 = [score['rouge-1']['f'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['f'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['f'] for score in scores]                 \n",
        " f1_score1.append(scores_rouge1[0])\n",
        " f1_score2.append(scores_rouge2[0])\n",
        " f1_scoreL.append(scores_rougeL[0])\n",
        " k=k+1\n",
        "\n",
        "print(\"the average f1_score for rouge-1 is\",math.fsum(f1_score1)/len(f1_score1))\n",
        "print(\"the average precision for rouge-1 is\",math.fsum(precision1)/len(precision1))\n",
        "print(\"the average recall for rouge-1 is\",math.fsum(recall1)/len(recall1))\n",
        "print(\"the average f1_score for rouge-2 is\",math.fsum(f1_score2)/len(f1_score2))\n",
        "print(\"the average precision for rouge-2 is\",math.fsum(precision2)/len(precision2))\n",
        "print(\"the average recall for rouge-2 is\",math.fsum(recall2)/len(recall2))\n",
        "print(\"the average f1_score for rouge-L is\",math.fsum(f1_scoreL)/len(f1_scoreL))\n",
        "print(\"the average precision for rouge-L is\",math.fsum(precisionL)/len(precisionL))\n",
        "print(\"the average recall for rouge-L is\",math.fsum(recallL)/len(recallL))\n",
        " \n",
        "\n",
        "\n",
        " \n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['102.txt', '104.txt', '105.txt', '106.txt', '108.txt', '109.txt', '110.txt', '111.txt', '118.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '148.txt', '149.txt', '150.txt', '151.txt', '152.txt', '153.txt', '155.txt', '156.txt', '159.txt', '160.txt', '161.txt', '163.txt', '168.txt', '169.txt', '17.txt', '170.txt', '173.txt', '174.txt', '181.txt', '19.txt', '205.txt', '206.txt', '214.txt', '225.txt', '227.txt', '228.txt', '229.txt', '230.txt', '64.txt', '65.txt', '66.txt', '67.txt', '72.txt', '73.txt', '92.txt', '93.txt']\n",
            "['102.txt', '104.txt', '105.txt', '106.txt', '108.txt', '109.txt', '110.txt', '111.txt', '118.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '148.txt', '149.txt', '150.txt', '151.txt', '152.txt', '153.txt', '155.txt', '156.txt', '159.txt', '160.txt', '161.txt', '163.txt', '168.txt', '169.txt', '17.txt', '170.txt', '173.txt', '174.txt', '181.txt', '19.txt', '205.txt', '206.txt', '214.txt', '225.txt', '227.txt', '228.txt', '229.txt', '230.txt', '64.txt', '65.txt', '66.txt', '67.txt', '72.txt', '73.txt', '92.txt', '93.txt']\n",
            "102.txt\n",
            "file read successfully\n",
            "102.txt\n",
            "file read successfully\n",
            "number of words in reference summary 5638\n",
            "number of words in text file 158030\n",
            "number of sentences in text file 674\n",
            "number of words in the model summary 3907\n",
            "104.txt\n",
            "file read successfully\n",
            "104.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4127\n",
            "number of words in text file 260640\n",
            "number of sentences in text file 1261\n",
            "number of words in the model summary 1899\n",
            "105.txt\n",
            "file read successfully\n",
            "105.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4498\n",
            "number of words in text file 264338\n",
            "number of sentences in text file 1177\n",
            "number of words in the model summary 2263\n",
            "106.txt\n",
            "file read successfully\n",
            "106.txt\n",
            "file read successfully\n",
            "number of words in reference summary 5380\n",
            "number of words in text file 249349\n",
            "number of sentences in text file 1122\n",
            "number of words in the model summary 2232\n",
            "108.txt\n",
            "file read successfully\n",
            "108.txt\n",
            "file read successfully\n",
            "number of words in reference summary 7358\n",
            "number of words in text file 249810\n",
            "number of sentences in text file 1165\n",
            "number of words in the model summary 2158\n",
            "109.txt\n",
            "file read successfully\n",
            "109.txt\n",
            "file read successfully\n",
            "number of words in reference summary 3456\n",
            "number of words in text file 254976\n",
            "number of sentences in text file 1104\n",
            "number of words in the model summary 2203\n",
            "110.txt\n",
            "file read successfully\n",
            "110.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4075\n",
            "number of words in text file 311819\n",
            "number of sentences in text file 1280\n",
            "number of words in the model summary 2984\n",
            "111.txt\n",
            "file read successfully\n",
            "111.txt\n",
            "file read successfully\n",
            "number of words in reference summary 6785\n",
            "number of words in text file 328443\n",
            "number of sentences in text file 1323\n",
            "number of words in the model summary 3184\n",
            "118.txt\n",
            "file read successfully\n",
            "118.txt\n",
            "file read successfully\n",
            "number of words in reference summary 847\n",
            "number of words in text file 189828\n",
            "number of sentences in text file 886\n",
            "number of words in the model summary 2312\n",
            "128.txt\n",
            "file read successfully\n",
            "128.txt\n",
            "file read successfully\n",
            "number of words in reference summary 2959\n",
            "number of words in text file 139845\n",
            "number of sentences in text file 608\n",
            "number of words in the model summary 3971\n",
            "the average f1_score for rouge-1 is 0.2811713299246297\n",
            "the average precision for rouge-1 is 0.39843842756543213\n",
            "the average recall for rouge-1 is 0.24296012816889406\n",
            "the average f1_score for rouge-2 is 0.049628487490854065\n",
            "the average precision for rouge-2 is 0.07205116762759178\n",
            "the average recall for rouge-2 is 0.04177028859905876\n",
            "the average f1_score for rouge-L is 0.18569321926085208\n",
            "the average precision for rouge-L is 0.23808014285231965\n",
            "the average recall for rouge-L is 0.16081766961981248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWUIvsXtFSif",
        "outputId": "48d55b21-4de1-4384-fa59-5f9cac98a51c"
      },
      "source": [
        "print(f1_score1)\n",
        "print(f1_score2)\n",
        "print(f1_scoreL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.37414965497593594, 0.26897213805210207, 0.31940575223290396, 0.23773898171520497, 0.26298487486199135, 0.3126385762677667, 0.2532832972505096, 0.33461046821795043, 0.1285444196575914, 0.3193851360143403]\n",
            "[0.07356947740112439, 0.04042348000385398, 0.09302325131706242, 0.029975016665352308, 0.060566158445080974, 0.03555555089355617, 0.026315784661107355, 0.06521738698252265, 0.015180261881043246, 0.056458506657837144]\n",
            "[0.22963951447701528, 0.2256809292555528, 0.2560296798614903, 0.16293929262481, 0.18131101398901756, 0.16867469398880677, 0.15607984996439422, 0.18104667154152568, 0.1137123700656594, 0.181818176840249]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIVddjydmCHq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVwaaM_Rre4S"
      },
      "source": [
        "from rouge import Rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTv58cobwlxM"
      },
      "source": [
        "rouge = Rouge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2Un_ay_wkP4",
        "outputId": "cd8cec86-165f-48c1-f7b2-624012422c69"
      },
      "source": [
        "print(len(model_out))\n",
        "print(len(reference))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5679\n",
            "5638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8rKlM4CwnoX",
        "outputId": "1bb4891a-0048-4962-a09e-e14f7a8f06d1"
      },
      "source": [
        "rouge.get_scores([model_out], [reference], avg=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.4349334825676105,\n",
              "  'p': 0.4253393665158371,\n",
              "  'r': 0.44497041420118344},\n",
              " 'rouge-2': {'f': 0.09264620229843894,\n",
              "  'p': 0.09060022650056625,\n",
              "  'r': 0.0947867298578199},\n",
              " 'rouge-l': {'f': 0.27681660399659974,\n",
              "  'p': 0.27586206896551724,\n",
              "  'r': 0.2777777777777778}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}