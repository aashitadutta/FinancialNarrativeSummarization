{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-idf-summarization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashitadutta/FinancialNarrativeSummarization/blob/main/tf_idf_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM3JwXyMYpzx",
        "outputId": "6af32f46-8052-453c-e652-63608ddfbe71"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbfQA9DvZ6r-"
      },
      "source": [
        "\n",
        "### importing the necessary libraries\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas\n",
        "import nltk\n",
        "import re\n",
        "from nltk import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY8BSltSaBV0"
      },
      "source": [
        "inp_data=\"/content/drive/MyDrive/lstm-input/\"\n",
        "out_data=\"/content/drive/MyDrive/lstm-summ/\"\n",
        "\n",
        "datasets={\"text\":inp_data,\"summ\":out_data}\n",
        "\n",
        "data_categories=[\"training\",\"validation\",\"test\"]\n",
        "\n",
        "data={\"articles\":[],\"summaries\":[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DUZVM1xi3UC",
        "outputId": "b17d5d68-311d-417f-a166-7c6765b952ee"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-YZuunWkXUV",
        "outputId": "9ac47268-8199-48bb-c524-7a02c9f4cca8"
      },
      "source": [
        "!pip install Rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Rouge) (1.15.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY51Sb-Nkdlz",
        "outputId": "82dcefbd-4576-4671-fd93-74e2c35b297d"
      },
      "source": [
        "!pip install rouge_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4ihCIBvoTgC"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0O22DTBiyfC",
        "outputId": "94252d60-9c59-4181-cc64-6c08d93eea69"
      },
      "source": [
        "def parsetext(dire,category,filename):\n",
        "    with open(\"%s/%s\"%(dire,filename),'r',encoding=\"Latin-1\") as readin:\n",
        "        print(\"file read successfully\")\n",
        "        text=readin.read()\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def load_data(dire,category):\n",
        "    for (root,dirs,files) in os.walk(dire):\n",
        "         filenames=sorted(files)\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def cleantext(text):\n",
        "    text=re.sub(r\"what's\",\"what is \",text)\n",
        "    text=re.sub(r\"it's\",\"it is \",text)\n",
        "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
        "    text=re.sub(r\"i'm\",\"i am \",text)\n",
        "    text=re.sub(r\"\\'re\",\" are \",text)\n",
        "    text=re.sub(r\"n't\",\" not \",text)\n",
        "    text=re.sub(r\"\\'d\",\" would \",text)\n",
        "    text=re.sub(r\"\\'s\",\"s\",text)\n",
        "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
        "    text=re.sub(r\"can't\",\" cannot \",text)\n",
        "    text=re.sub(r\" e g \",\" eg \",text)\n",
        "    text=re.sub(r\"e-mail\",\"email\",text)\n",
        "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
        "    text=re.sub(r\" u.s\",\" american \",text)\n",
        "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
        "    text=re.sub(r\"\\n\",\" \",text)\n",
        "    text=re.sub(r\":\",\" \",text)\n",
        "    text=re.sub(r\"-\",\" \",text)\n",
        "    text=re.sub(r\"\\_\",\" \",text)\n",
        "    text=re.sub(r\"\\d+\",\" \",text)\n",
        "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def printArticlesum(k):\n",
        "    print(\"---------------------original sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"articles\"][k])\n",
        "    print(\"----------------------Summary sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"summaries\"][k])\n",
        "    return 0\n",
        "\n",
        "\n",
        "def announcedone():\n",
        "    duration=2000\n",
        "    freq=440\n",
        "    ws.Beep(freq,duration)\n",
        "\n",
        "dict1={}\n",
        "f1_score1=[]\n",
        "precision1=[]\n",
        "recall1=[]\n",
        "f1_score2=[]\n",
        "precision2=[]\n",
        "recall2=[]\n",
        "f1_scoreL=[]\n",
        "precisionL=[]\n",
        "recallL=[]\n",
        "filenames1=load_data(datasets[\"text\"],data_categories[0])\n",
        "filenames2=load_data(datasets[\"summ\"],data_categories[0])\n",
        "print(filenames1)\n",
        "print(filenames2)\n",
        "\n",
        "\"\"\"----------load the data, sentences and summaries-----------\"\"\"\n",
        "data[\"articles\"]=[]\n",
        "data[\"summaries\"]=[]\n",
        "k=0\n",
        "while k<10:\n",
        " try:\n",
        "    print(filenames1[k])\n",
        "    data[\"articles\"].append(parsetext(datasets[\"text\"],data_categories[0],\"%s\"%filenames1[k]))\n",
        " except Exception as e:\n",
        "    data[\"articles\"].append(\"Could not read\")\n",
        "    print(e)\n",
        " try:\n",
        "   print(filenames2[k])\n",
        "   data[\"summaries\"].append(parsetext(datasets[\"summ\"],data_categories[0],\"%s\"%filenames2[k]))\n",
        " except Exception as e:\n",
        "   data[\"summaries\"].append(\"Could not read\")\n",
        "   print(e)\n",
        " \n",
        " reference=\"\"\n",
        " reference=data[\"summaries\"][k]\n",
        " print(\"number of words in reference summary\",len(reference))\n",
        " s = \"\"\n",
        " for a in data['articles'][k]:\n",
        "      s += a\n",
        " print(\"number of words in text file\",len(s))\n",
        " sentences = sent_tokenize(s)\n",
        " print(\"number of sentences in text file\",len(sentences))\n",
        " \n",
        " dict = {}\n",
        " text=\"\"\n",
        " for a in sentences:\n",
        "    temp = re.sub(\"[^a-zA-Z]\",\" \",a)\n",
        "    temp = temp.lower()\n",
        "    dict[temp] = a\n",
        "    text+=temp\n",
        " stopwords = nltk.corpus.stopwords.words('english')\n",
        " word_frequencies = {}\n",
        " for word in nltk.word_tokenize(text):\n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        " max_freq = max(word_frequencies.values())\n",
        "\n",
        " for w in word_frequencies :\n",
        "      word_frequencies[w]/=max_freq\n",
        " sentence_scores = {}\n",
        " for sent in sentences:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]\n",
        " import heapq\n",
        " summary_sentences = heapq.nlargest(5, sentence_scores, key=sentence_scores.get)\n",
        " summary = ' '.join(summary_sentences)\n",
        " #print(summary)\n",
        " from rouge import Rouge\n",
        " print(\"number of words in the model summary\",len(summary))\n",
        " rouge = Rouge()\n",
        " scores = rouge.get_scores(summary, reference)\n",
        " scores_rouge1 = [score['rouge-1']['r'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['r'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['r'] for score in scores]                 \n",
        " recall1.append(scores_rouge1[0])\n",
        " recall2.append(scores_rouge2[0])\n",
        " recallL.append(scores_rougeL[0])\n",
        " scores_rouge1 = [score['rouge-1']['p'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['p'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['p'] for score in scores]                 \n",
        " precision1.append(scores_rouge1[0])\n",
        " precision2.append(scores_rouge2[0])\n",
        " precisionL.append(scores_rougeL[0])\n",
        " scores_rouge1 = [score['rouge-1']['f'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['f'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['f'] for score in scores]                 \n",
        " f1_score1.append(scores_rouge1[0])\n",
        " f1_score2.append(scores_rouge2[0])\n",
        " f1_scoreL.append(scores_rougeL[0])\n",
        " k=k+1\n",
        "\n",
        "print(\"the average f1_score for rouge-1 is\",math.fsum(f1_score1)/len(f1_score1))\n",
        "print(\"the average precision for rouge-1 is\",math.fsum(precision1)/len(precision1))\n",
        "print(\"the average recall for rouge-1 is\",math.fsum(recall1)/len(recall1))\n",
        "print(\"the average f1_score for rouge-2 is\",math.fsum(f1_score2)/len(f1_score2))\n",
        "print(\"the average precision for rouge-2 is\",math.fsum(precision2)/len(precision2))\n",
        "print(\"the average recall for rouge-2 is\",math.fsum(recall2)/len(recall2))\n",
        "print(\"the average f1_score for rouge-L is\",math.fsum(f1_scoreL)/len(f1_scoreL))\n",
        "print(\"the average precision for rouge-L is\",math.fsum(precisionL)/len(precisionL))\n",
        "print(\"the average recall for rouge-L is\",math.fsum(recallL)/len(recallL))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['102.txt', '104.txt', '105.txt', '106.txt', '108.txt', '109.txt', '110.txt', '111.txt', '118.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '148.txt', '149.txt', '150.txt', '151.txt', '152.txt', '153.txt', '155.txt', '156.txt', '159.txt', '160.txt', '161.txt', '163.txt', '168.txt', '169.txt', '17.txt', '170.txt', '173.txt', '174.txt', '181.txt', '19.txt', '205.txt', '206.txt', '214.txt', '225.txt', '227.txt', '228.txt', '229.txt', '230.txt', '64.txt', '65.txt', '66.txt', '67.txt', '72.txt', '73.txt', '92.txt', '93.txt']\n",
            "['102.txt', '104.txt', '105.txt', '106.txt', '108.txt', '109.txt', '110.txt', '111.txt', '118.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '148.txt', '149.txt', '150.txt', '151.txt', '152.txt', '153.txt', '155.txt', '156.txt', '159.txt', '160.txt', '161.txt', '163.txt', '168.txt', '169.txt', '17.txt', '170.txt', '173.txt', '174.txt', '181.txt', '19.txt', '205.txt', '206.txt', '214.txt', '225.txt', '227.txt', '228.txt', '229.txt', '230.txt', '64.txt', '65.txt', '66.txt', '67.txt', '72.txt', '73.txt', '92.txt', '93.txt']\n",
            "102.txt\n",
            "file read successfully\n",
            "102.txt\n",
            "file read successfully\n",
            "number of words in reference summary 5638\n",
            "number of words in text file 158030\n",
            "number of sentences in text file 674\n",
            "number of words in the model summary 1006\n",
            "104.txt\n",
            "file read successfully\n",
            "104.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4127\n",
            "number of words in text file 260640\n",
            "number of sentences in text file 1261\n",
            "number of words in the model summary 521\n",
            "105.txt\n",
            "file read successfully\n",
            "105.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4498\n",
            "number of words in text file 264338\n",
            "number of sentences in text file 1177\n",
            "number of words in the model summary 857\n",
            "106.txt\n",
            "file read successfully\n",
            "106.txt\n",
            "file read successfully\n",
            "number of words in reference summary 5380\n",
            "number of words in text file 249349\n",
            "number of sentences in text file 1122\n",
            "number of words in the model summary 855\n",
            "108.txt\n",
            "file read successfully\n",
            "108.txt\n",
            "file read successfully\n",
            "number of words in reference summary 7358\n",
            "number of words in text file 249810\n",
            "number of sentences in text file 1165\n",
            "number of words in the model summary 847\n",
            "109.txt\n",
            "file read successfully\n",
            "109.txt\n",
            "file read successfully\n",
            "number of words in reference summary 3456\n",
            "number of words in text file 254976\n",
            "number of sentences in text file 1104\n",
            "number of words in the model summary 790\n",
            "110.txt\n",
            "file read successfully\n",
            "110.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4075\n",
            "number of words in text file 311819\n",
            "number of sentences in text file 1280\n",
            "number of words in the model summary 799\n",
            "111.txt\n",
            "file read successfully\n",
            "111.txt\n",
            "file read successfully\n",
            "number of words in reference summary 6785\n",
            "number of words in text file 328443\n",
            "number of sentences in text file 1323\n",
            "number of words in the model summary 856\n",
            "118.txt\n",
            "file read successfully\n",
            "118.txt\n",
            "file read successfully\n",
            "number of words in reference summary 847\n",
            "number of words in text file 189828\n",
            "number of sentences in text file 886\n",
            "number of words in the model summary 1142\n",
            "128.txt\n",
            "file read successfully\n",
            "128.txt\n",
            "file read successfully\n",
            "number of words in reference summary 2959\n",
            "number of words in text file 139845\n",
            "number of sentences in text file 608\n",
            "number of words in the model summary 707\n",
            "the average f1_score for rouge-1 is 0.16543784601018047\n",
            "the average precision for rouge-1 is 0.4960030008115366\n",
            "the average recall for rouge-1 is 0.1109529432347042\n",
            "the average f1_score for rouge-2 is 0.06169542595250619\n",
            "the average precision for rouge-2 is 0.19280555796095267\n",
            "the average recall for rouge-2 is 0.04137634638952771\n",
            "the average f1_score for rouge-L is 0.16374689566993844\n",
            "the average precision for rouge-L is 0.46211529113508404\n",
            "the average recall for rouge-L is 0.10496071535402371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGhSBoJMHRHr",
        "outputId": "5323ed53-7cb4-44ce-c38d-71e2d1e80c9b"
      },
      "source": [
        "print(f1_score1)\n",
        "print(f1_score2)\n",
        "print(f1_scoreL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.1352169500607384, 0.16627078170070694, 0.1742243410705966, 0.17248459735199373, 0.09429657605616394, 0.1296829941462848, 0.15665796062826803, 0.12151067111233796, 0.2918149416299186, 0.21221864634479587]\n",
            "[0.048533870096235665, 0.11190475977108846, 0.0789473658277055, 0.06790123234195336, 0.04874333399092451, 0.005780343860680121, 0.0366492118529168, 0.029605261044894977, 0.1146953355126478, 0.0741935452260147]\n",
            "[0.1739130404834174, 0.1721518959846179, 0.16624684861321376, 0.21327967523288627, 0.09208103006949321, 0.10869564909321841, 0.12935323070320048, 0.11851851596851859, 0.28571428097009643, 0.177514789580722]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}