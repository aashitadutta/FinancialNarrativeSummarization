{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRank-summarization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashitadutta/FinancialNarrativeSummarization/blob/main/TextRank_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_eKSMk7ipD4",
        "outputId": "78da4f56-dfde-4610-8f97-e2c73f013dbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s4wDdXrh3XU"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import pandas\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1s6b8Wmlisj",
        "outputId": "ebc838a0-9cfe-4ad2-d3b0-a83c9da3d5cc"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk1v_fa0in8Y"
      },
      "source": [
        "inp_data=\"/content/drive/MyDrive/lstm-input/\"\n",
        "out_data=\"/content/drive/MyDrive/lstm-summ/\"\n",
        "\n",
        "datasets={\"text\":inp_data,\"summ\":out_data}\n",
        "\n",
        "data_categories=[\"training\",\"validation\",\"test\"]\n",
        "\n",
        "data={\"articles\":[],\"summaries\":[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnwW7B2hm0ZE",
        "outputId": "081a826d-feb8-4f68-f2cd-2788388995d4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOA5fvIGm6UM",
        "outputId": "6042fa92-6998-4859-a1dd-40a502ba7d5b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuHuLwoBnAID"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ1F3vP0oyPZ",
        "outputId": "0be58172-609c-48a4-90f3-24948225922c"
      },
      "source": [
        "!pip install Rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Rouge) (1.15.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgStt6eLBURu",
        "outputId": "377f3de5-9ede-49f8-9a2a-30bf59ca8cb4"
      },
      "source": [
        "!pip install rouge_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeGHpuaU0Rzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bf9955-1c4e-4ce3-b981-a83a140abea1"
      },
      "source": [
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-28 10:11:57--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.224.187\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.224.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz.1’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  39.3MB/s    in 39s     \n",
            "\n",
            "2021-06-28 10:12:37 (39.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz.1’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNAYykt30VIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d801ce-6eca-4753-8bfb-450ea493765b"
      },
      "source": [
        "!gzip -d GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twFyiEp62GWk"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP4D6XW3jKBD",
        "outputId": "d787ed50-bba8-4ba3-9e4a-016cd6560961"
      },
      "source": [
        "def parsetext(dire,category,filename):\n",
        "    with open(\"%s/%s\"%(dire,filename),'r',encoding=\"Latin-1\") as readin:\n",
        "        print(\"file read successfully\")\n",
        "        text=readin.read()\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def load_data(dire,category):\n",
        "    for (root,dirs,files) in os.walk(dire):\n",
        "         filenames=sorted(files)\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def cleantext(text):\n",
        "    text=re.sub(r\"what's\",\"what is \",text)\n",
        "    text=re.sub(r\"it's\",\"it is \",text)\n",
        "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
        "    text=re.sub(r\"i'm\",\"i am \",text)\n",
        "    text=re.sub(r\"\\'re\",\" are \",text)\n",
        "    text=re.sub(r\"n't\",\" not \",text)\n",
        "    text=re.sub(r\"\\'d\",\" would \",text)\n",
        "    text=re.sub(r\"\\'s\",\"s\",text)\n",
        "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
        "    text=re.sub(r\"can't\",\" cannot \",text)\n",
        "    text=re.sub(r\" e g \",\" eg \",text)\n",
        "    text=re.sub(r\"e-mail\",\"email\",text)\n",
        "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
        "    text=re.sub(r\" u.s\",\" american \",text)\n",
        "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
        "    text=re.sub(r\"\\n\",\" \",text)\n",
        "    text=re.sub(r\":\",\" \",text)\n",
        "    text=re.sub(r\"-\",\" \",text)\n",
        "    text=re.sub(r\"\\_\",\" \",text)\n",
        "    text=re.sub(r\"\\d+\",\" \",text)\n",
        "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def printArticlesum(k):\n",
        "    print(\"---------------------original sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"articles\"][k])\n",
        "    print(\"----------------------Summary sentence-----------------------\")\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(data[\"summaries\"][k])\n",
        "    return 0\n",
        "\n",
        "\n",
        "def announcedone():\n",
        "    duration=2000\n",
        "    freq=440\n",
        "    ws.Beep(freq,duration)\n",
        "\n",
        "dict1={}\n",
        "f1_score1=[]\n",
        "precision1=[]\n",
        "recall1=[]\n",
        "f1_score2=[]\n",
        "precision2=[]\n",
        "recall2=[]\n",
        "f1_scoreL=[]\n",
        "precisionL=[]\n",
        "recallL=[]\n",
        "filenames1=load_data(datasets[\"text\"],data_categories[0])\n",
        "filenames2=load_data(datasets[\"summ\"],data_categories[0])\n",
        "print(filenames1)\n",
        "print(filenames2)\n",
        "\n",
        "\"\"\"----------load the data, sentences and summaries-----------\"\"\"\n",
        "data[\"articles\"]=[]\n",
        "data[\"summaries\"]=[]\n",
        "k=0\n",
        "while k<10:\n",
        " try:\n",
        "    print(filenames1[k])\n",
        "    data[\"articles\"].append(parsetext(datasets[\"text\"],data_categories[0],\"%s\"%filenames1[k]))\n",
        " except Exception as e:\n",
        "    data[\"articles\"].append(\"Could not read\")\n",
        "    print(e)\n",
        " try:\n",
        "   print(filenames2[k])\n",
        "   data[\"summaries\"].append(parsetext(datasets[\"summ\"],data_categories[0],\"%s\"%filenames2[k]))\n",
        " except Exception as e:\n",
        "   data[\"summaries\"].append(\"Could not read\")\n",
        "   print(e)\n",
        " \n",
        " reference=\"\"\n",
        " reference=data[\"summaries\"][k]\n",
        " print(\"number of words in reference summary\",len(reference))\n",
        " s = \"\"\n",
        " d = {}\n",
        " for a in data['articles'][k]:\n",
        "      s += a\n",
        " #print(\"....\")\n",
        " print(\"number of words in text file\",len(s))\n",
        " # print s\n",
        " sentences = sent_tokenize(s)\n",
        " print(\"number of sentences in text file\",len(sentences))\n",
        " clean_sentences = []\n",
        " for s in sentences:\n",
        "    temp = re.sub(\"[^a-zA-Z]\",\" \",s)\n",
        "    temp = temp.lower()\n",
        "    clean_sentences.append(temp)\n",
        "    d[temp] = s \n",
        " #print(len(clean_sentences))\n",
        " stop_words = stopwords.words('english')\n",
        " def rem_stop(s):\n",
        "    var = \"\"\n",
        "    words = nltk.word_tokenize(s)\n",
        "    for w in words:\n",
        "        if( w not in stop_words):\n",
        "           var+=w+\" \"\n",
        "    return var\n",
        " dict = {}\n",
        " clean = []\n",
        " # print clean_sentences\n",
        " for s in clean_sentences:\n",
        "    temp = rem_stop(s)\n",
        "    clean.append(temp)\n",
        "    dict[temp] = d[s]\n",
        " #print(len(clean))\n",
        " ### creating vector representation of sentences after extracting word vectors\n",
        " \n",
        " # print(model)\n",
        " word_embeddings = {}\n",
        " words = list(model.wv.vocab)\n",
        " # print len(words)\n",
        " for a in words:\n",
        "    word_embeddings[a]=model[a]\n",
        " \n",
        " # print len(word_embeddings)\n",
        " \n",
        " \n",
        " sentence_vectors = []\n",
        " for i in clean:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((300,))\n",
        "  sentence_vectors.append(v)\n",
        " #print(len(sentence_vectors))\n",
        " #print(len(sentence_vectors[1]))\n",
        " \n",
        " sentence_similarity_martix = np.zeros([len(sentences), len(sentences)])\n",
        " for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sentence_similarity_martix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
        " \n",
        " sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        " scores = nx.pagerank(sentence_similarity_graph)\n",
        " ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(clean)), reverse=True)\n",
        " model_out=\"\"\n",
        " for i in range(5):\n",
        "     model_out+=dict[ranked_sentence[i][1]]\n",
        "     #print(dict[ranked_sentence[i][1]])\n",
        " from rouge import Rouge\n",
        " print(\"number of words in the model summary\",len(model_out))\n",
        " rouge = Rouge()\n",
        " scores = rouge.get_scores(model_out, reference)\n",
        " scores_rouge1 = [score['rouge-1']['r'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['r'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['r'] for score in scores]                 \n",
        " recall1.append(scores_rouge1[0])\n",
        " recall2.append(scores_rouge2[0])\n",
        " recallL.append(scores_rougeL[0])\n",
        " scores_rouge1 = [score['rouge-1']['p'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['p'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['p'] for score in scores]                 \n",
        " precision1.append(scores_rouge1[0])\n",
        " precision2.append(scores_rouge2[0])\n",
        " precisionL.append(scores_rougeL[0])\n",
        " scores_rouge1 = [score['rouge-1']['f'] for score in scores]\n",
        " scores_rouge2 = [score['rouge-2']['f'] for score in scores]\n",
        " scores_rougeL = [score['rouge-l']['f'] for score in scores]                 \n",
        " f1_score1.append(scores_rouge1[0])\n",
        " f1_score2.append(scores_rouge2[0])\n",
        " f1_scoreL.append(scores_rougeL[0])\n",
        " k=k+1\n",
        "\n",
        "print(\"the average f1_score for rouge-1 is\",math.fsum(f1_score1)/len(f1_score1))\n",
        "print(\"the average precision for rouge-1 is\",math.fsum(precision1)/len(precision1))\n",
        "print(\"the average recall for rouge-1 is\",math.fsum(recall1)/len(recall1))\n",
        "print(\"the average f1_score for rouge-2 is\",math.fsum(f1_score2)/len(f1_score2))\n",
        "print(\"the average precision for rouge-2 is\",math.fsum(precision2)/len(precision2))\n",
        "print(\"the average recall for rouge-2 is\",math.fsum(recall2)/len(recall2))\n",
        "print(\"the average f1_score for rouge-L is\",math.fsum(f1_scoreL)/len(f1_scoreL))\n",
        "print(\"the average precision for rouge-L is\",math.fsum(precisionL)/len(precisionL))\n",
        "print(\"the average recall for rouge-L is\",math.fsum(recallL)/len(recallL))\n",
        " \n",
        " \n",
        "\n",
        " \n",
        "\n",
        "\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['102.txt', '104.txt', '105.txt', '106.txt', '108.txt', '109.txt', '110.txt', '111.txt', '118.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '148.txt', '149.txt', '150.txt', '151.txt', '152.txt', '153.txt', '155.txt', '156.txt', '159.txt', '160.txt', '161.txt', '163.txt', '168.txt', '169.txt', '17.txt', '170.txt', '173.txt', '174.txt', '181.txt', '19.txt', '205.txt', '206.txt', '214.txt', '225.txt', '227.txt', '228.txt', '229.txt', '230.txt', '64.txt', '65.txt', '66.txt', '67.txt', '72.txt', '73.txt', '92.txt', '93.txt']\n",
            "['102.txt', '104.txt', '105.txt', '106.txt', '108.txt', '109.txt', '110.txt', '111.txt', '118.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '148.txt', '149.txt', '150.txt', '151.txt', '152.txt', '153.txt', '155.txt', '156.txt', '159.txt', '160.txt', '161.txt', '163.txt', '168.txt', '169.txt', '17.txt', '170.txt', '173.txt', '174.txt', '181.txt', '19.txt', '205.txt', '206.txt', '214.txt', '225.txt', '227.txt', '228.txt', '229.txt', '230.txt', '64.txt', '65.txt', '66.txt', '67.txt', '72.txt', '73.txt', '92.txt', '93.txt']\n",
            "102.txt\n",
            "file read successfully\n",
            "102.txt\n",
            "file read successfully\n",
            "number of words in reference summary 5638\n",
            "number of words in text file 158030\n",
            "number of sentences in text file 674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 6823\n",
            "104.txt\n",
            "file read successfully\n",
            "104.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4127\n",
            "number of words in text file 260640\n",
            "number of sentences in text file 1261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 3283\n",
            "105.txt\n",
            "file read successfully\n",
            "105.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4498\n",
            "number of words in text file 264338\n",
            "number of sentences in text file 1177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 4941\n",
            "106.txt\n",
            "file read successfully\n",
            "106.txt\n",
            "file read successfully\n",
            "number of words in reference summary 5380\n",
            "number of words in text file 249349\n",
            "number of sentences in text file 1122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 5203\n",
            "108.txt\n",
            "file read successfully\n",
            "108.txt\n",
            "file read successfully\n",
            "number of words in reference summary 7358\n",
            "number of words in text file 249810\n",
            "number of sentences in text file 1165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 6243\n",
            "109.txt\n",
            "file read successfully\n",
            "109.txt\n",
            "file read successfully\n",
            "number of words in reference summary 3456\n",
            "number of words in text file 254976\n",
            "number of sentences in text file 1104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 5209\n",
            "110.txt\n",
            "file read successfully\n",
            "110.txt\n",
            "file read successfully\n",
            "number of words in reference summary 4075\n",
            "number of words in text file 311819\n",
            "number of sentences in text file 1280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 3975\n",
            "111.txt\n",
            "file read successfully\n",
            "111.txt\n",
            "file read successfully\n",
            "number of words in reference summary 6785\n",
            "number of words in text file 328443\n",
            "number of sentences in text file 1323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 5837\n",
            "118.txt\n",
            "file read successfully\n",
            "118.txt\n",
            "file read successfully\n",
            "number of words in reference summary 847\n",
            "number of words in text file 189828\n",
            "number of sentences in text file 886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 9449\n",
            "128.txt\n",
            "file read successfully\n",
            "128.txt\n",
            "file read successfully\n",
            "number of words in reference summary 2959\n",
            "number of words in text file 139845\n",
            "number of sentences in text file 608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of words in the model summary 3963\n",
            "the average f1_score for rouge-1 is 0.28985594188232283\n",
            "the average precision for rouge-1 is 0.29258874317650096\n",
            "the average recall for rouge-1 is 0.33048813994336784\n",
            "the average f1_score for rouge-2 is 0.07129852096644654\n",
            "the average precision for rouge-2 is 0.07131756504431473\n",
            "the average recall for rouge-2 is 0.08329877025237022\n",
            "the average f1_score for rouge-L is 0.20325078663677063\n",
            "the average precision for rouge-L is 0.20027833396741088\n",
            "the average recall for rouge-L is 0.22641012458502777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjno8XWwFnGJ",
        "outputId": "ddd1574a-d10c-47d5-b0f8-d27aca7d372b"
      },
      "source": [
        "print(f1_score1)\n",
        "print(f1_score2)\n",
        "print(f1_scoreL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.3245663072699084, 0.29623567443383847, 0.3420329620363728, 0.31786394925916484, 0.25554483597700894, 0.37508845945259095, 0.28548123481128035, 0.317220538837497, 0.07535515598629772, 0.30917030075926855]\n",
            "[0.07619047120524164, 0.05409835587589401, 0.07290233338030684, 0.09166135722134303, 0.07528957037958339, 0.12898652956548268, 0.04738561592209513, 0.08770160793413873, 0.02102659094544716, 0.05774277723493267]\n",
            "[0.20247468566762125, 0.23294508656232968, 0.22773722128300933, 0.20769230270821842, 0.18259022857485782, 0.29729729235253277, 0.17610062393259773, 0.2321839030611971, 0.09315865829526945, 0.1803278639300727]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}